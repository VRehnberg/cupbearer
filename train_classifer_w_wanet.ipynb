{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/Arch/software/tqdm/4.64.1-GCCcore-12.2.0/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from cupbearer import data, detectors, models, scripts, tasks, utils\n",
    "from lightning.pytorch import loggers\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from tensorboard import notebook\n",
    "import torch\n",
    "import submitit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a backdoored classifier\n",
    "First, we train a classifier on poisoned data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = submitit.SlurmExecutor(folder=\"runs\")\n",
    "executor.update_parameters(\n",
    "    account=\"NAISS2023-22-1064\",\n",
    "    gpus_per_node=\"A100:1\",\n",
    "    time=2*24*60,  # minutes\n",
    ")\n",
    "jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SlurmExecutor in module submitit.slurm.slurm object:\n",
      "\n",
      "class SlurmExecutor(submitit.core.core.PicklingExecutor)\n",
      " |  SlurmExecutor(folder: Union[pathlib.Path, str], max_num_timeout: int = 3, python: Optional[str] = None) -> None\n",
      " |  \n",
      " |  Slurm job executor\n",
      " |  This class is used to hold the parameters to run a job on slurm.\n",
      " |  In practice, it will create a batch file in the specified directory for each job,\n",
      " |  and pickle the task function and parameters. At completion, the job will also pickle\n",
      " |  the output. Logs are also dumped in the same directory.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  folder: Path/str\n",
      " |      folder for storing job submission/output and logs.\n",
      " |  max_num_timeout: int\n",
      " |      Maximum number of time the job can be requeued after timeout (if\n",
      " |      the instance is derived from helpers.Checkpointable)\n",
      " |  python: Optional[str]\n",
      " |      Command to launch python. This allow to use singularity for example.\n",
      " |      Caller is responsible to provide a valid shell command here.\n",
      " |      By default reuse the current python executable\n",
      " |  \n",
      " |  Note\n",
      " |  ----\n",
      " |  - be aware that the log/output folder will be full of logs and pickled objects very fast,\n",
      " |    it may need cleaning.\n",
      " |  - the folder needs to point to a directory shared through the cluster. This is typically\n",
      " |    not the case for your tmp! If you try to use it, slurm will fail silently (since it\n",
      " |    will not even be able to log stderr.\n",
      " |  - use update_parameters to specify custom parameters (n_gpus etc...). If you\n",
      " |    input erroneous parameters, an error will print all parameters available for you.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SlurmExecutor\n",
      " |      submitit.core.core.PicklingExecutor\n",
      " |      submitit.core.core.Executor\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, folder: Union[pathlib.Path, str], max_num_timeout: int = 3, python: Optional[str] = None) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  affinity() -> int from abc.ABCMeta\n",
      " |      The 'score' of this executor on the current environment.\n",
      " |      \n",
      " |      -> -1 means unavailable\n",
      " |      ->  0 means available but won't be started unless asked (eg debug executor)\n",
      " |      ->  1 means available\n",
      " |      ->  2 means available and is a highly scalable executor (cluster)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  job_class = <class 'submitit.slurm.slurm.SlurmJob'>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from submitit.core.core.Executor:\n",
      " |  \n",
      " |  batch(self, allow_implicit_submissions: bool = False) -> Iterator[NoneType]\n",
      " |      Creates a context within which all submissions are packed into a job array.\n",
      " |      By default the array submissions happens when leaving the context\n",
      " |      \n",
      " |      Parameter\n",
      " |      ---------\n",
      " |      allow_implicit_submissions: bool\n",
      " |          submits the current batch whenever a job attribute is accessed instead of raising an exception\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      jobs = []\n",
      " |      with executor.batch():\n",
      " |          for k in range(12):\n",
      " |              jobs.append(executor.submit(add, k, 1))\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          if trying to access a job instance attribute while the batch is not exited, and\n",
      " |          intermediate submissions are not allowed.\n",
      " |  \n",
      " |  map_array(self, fn: Callable[..., +R], *iterable: Iterable[Any]) -> List[submitit.core.core.Job[+R]]\n",
      " |      A distributed equivalent of the map() built-in function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fn: callable\n",
      " |          function to compute\n",
      " |      *iterable: Iterable\n",
      " |          lists of arguments that are passed as arguments to fn.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List[Job]\n",
      " |          A list of Job instances.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      a = [1, 2, 3]\n",
      " |      b = [10, 20, 30]\n",
      " |      executor.map_array(add, a, b)\n",
      " |      # jobs will compute 1 + 10, 2 + 20, 3 + 30\n",
      " |  \n",
      " |  submit(self, fn: Callable[..., +R], *args: Any, **kwargs: Any) -> submitit.core.core.Job[+R]\n",
      " |  \n",
      " |  submit_array(self, fns: Sequence[Callable[[], +R]]) -> List[submitit.core.core.Job[+R]]\n",
      " |      Submit a list of job. This is useful when submiting different Checkpointable functions.\n",
      " |      Be mindful that all those functions will be run with the same requirements\n",
      " |      (cpus, gpus, timeout, ...). So try to make group of similar function calls.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fns: list of callable\n",
      " |          functions to compute. Those functions must not need any argument.\n",
      " |          Tyically those are \"Checkpointable\" instance whose arguments\n",
      " |          have been specified in the constructor, or partial functions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      List[Job]\n",
      " |          A list of Job instances.\n",
      " |      \n",
      " |      Example\n",
      " |      -------\n",
      " |      a_vals = [1, 2, 3]\n",
      " |      b_vals = [10, 20, 30]\n",
      " |      fns = [functools.partial(int.__add__, a, b) for (a, b) in zip (a_vals, b_vals)]\n",
      " |      executor.submit_array(fns)\n",
      " |      # jobs will compute 1 + 10, 2 + 20, 3 + 30\n",
      " |  \n",
      " |  update_parameters(self, **kwargs: Any) -> None\n",
      " |      Update submision parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from submitit.core.core.Executor:\n",
      " |  \n",
      " |  name() -> str from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from submitit.core.core.Executor:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__path__', '__file__', '__cached__', '__builtins__', 'transforms', '_shared', 'MixedData', 'TransformDataset', 'adversarial', 'AdversarialExampleDataset', 'make_adversarial_examples', 'backdoors', 'Backdoor', 'BackdoorDataset', 'CornerPixelBackdoor', 'NoiseBackdoor', 'WanetBackdoor', 'huggingface', 'IMDBDataset', 'pytorch', 'CIFAR10', 'GTSRB', 'MNIST', 'PytorchDataset', 'tampering', 'TamperingDataset', 'toy_ambiguous_features', 'ToyDataset', 'GaussianNoise', 'RandomCrop', 'RandomHorizontalFlip', 'RandomRotation', 'Resize', 'ToTensor', 'Transform'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "partial(data.CIFAR10)().transforms\n",
    "data.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "def get_dataloaders(\n",
    "    Dataset: type,\n",
    "    backdoor: data.Backdoor,\n",
    ") -> tuple[\n",
    "    torch.utils.data.Dataset,\n",
    "    torch.utils.data.DataLoader,\n",
    "    dict[torch.utils.data.DataLoader],\n",
    "]:\n",
    "    dataset = data.BackdoorDataset(\n",
    "        original=Dataset(train=True),\n",
    "        backdoor=backdoor,\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=12,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    # Val data\n",
    "    val_data={\n",
    "        \"clean\": Dataset(train=False),\n",
    "        #\"backdoor\": data.BackdoorDataset(\n",
    "        #    original=Dataset(train=False),\n",
    "        #    backdoor=type(backdoor)(p_backdoor=1),\n",
    "        #),\n",
    "        \"backdoor\": data.BackdoorDataset(\n",
    "            original=Dataset(train=False),\n",
    "            backdoor=(\n",
    "                backdoor.clone(p_backdoor=1, p_noise=0)\n",
    "                if type(backdoor) == data.WanetBackdoor\n",
    "                else type(backdoor)(p_backdoor=1)\n",
    "            ),\n",
    "        ),\n",
    "    }\n",
    "    if type(backdoor) == data.WanetBackdoor:\n",
    "        val_data[\"noisy\"] = data.BackdoorDataset(\n",
    "            original=Dataset(train=False),\n",
    "            backdoor=backdoor.clone(p_backdoor=0, p_noise=1),\n",
    "        )\n",
    "    val_loaders = {\n",
    "        k: torch.utils.data.DataLoader(\n",
    "            v,\n",
    "            batch_size=2048,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            persistent_workers=True,\n",
    "        ) for k, v in val_data.items()\n",
    "    }\n",
    "\n",
    "    return dataset, train_loader, val_loaders\n",
    "\n",
    "class WanetClassifier(scripts._shared.Classifier):\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            opt,\n",
    "            milestones=[100, 200, 300, 400],\n",
    "            gamma=0.1,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": opt,\n",
    "            \"lr_scheduler\": lr_scheduler,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model: torch.nn.Module, Dataset: type, backdoor: data.Backdoor):\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    \n",
    "    path = Path(f\"logs/{type(model).__name__}/{Dataset.__name__}/{type(backdoor).__name__}\")\n",
    "    cfg_path = path / os.getenv(\"SLURM_JOB_ID\", datetime.datetime.now().isoformat())\n",
    "\n",
    "    dataset, train_loader, val_loaders = get_dataloaders(Dataset, backdoor)\n",
    "\n",
    "    # Dataloader returns images and labels, only images get passed to model\n",
    "    images, _ = next(iter(train_loader))\n",
    "    example_input = images[0]\n",
    "    \n",
    "    classifier = WanetClassifier(\n",
    "        model=model,\n",
    "        lr=0.01,\n",
    "        #input_shape=example_input.shape,\n",
    "        num_classes=dataset.original.num_classes,\n",
    "        val_loader_names=list(val_loaders.keys()),\n",
    "    )\n",
    "\n",
    "    metrics_logger = loggers.TensorBoardLogger(\n",
    "        save_dir=cfg_path,\n",
    "        name=\"\",\n",
    "        version=\"\",\n",
    "        sub_dir=\"tensorboard\",\n",
    "    )\n",
    "    \n",
    "    # TODO: once we do longer training runs we'll want to have multiple\n",
    "    # checkpoints, potentially based on validation loss\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            dirpath=cfg_path / \"checkpoints\",\n",
    "            save_last=True,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        enable_progress_bar=False,\n",
    "        callbacks=callbacks,\n",
    "        logger=metrics_logger,\n",
    "        default_root_dir=cfg_path,\n",
    "        check_val_every_n_epoch=5,\n",
    "        #precision=32,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model=classifier,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=list(val_loaders.values()),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        dataset.backdoor.store(cfg_path)\n",
    "    except AttributeError as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-16 12:14:59.683\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.685\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.764\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.765\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.766\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.767\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.767\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.768\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.835\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.836\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.836\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.837\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.837\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.838\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.915\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.916\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.916\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.917\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m140\u001b[0m - \u001b[34m\u001b[1mGenerating new control grid for warping field.\u001b[0m\n",
      "\u001b[32m2024-05-16 12:14:59.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mcupbearer.data.backdoors\u001b[0m:\u001b[36mcontrol_grid\u001b[0m:\u001b[36m163\u001b[0m - \u001b[34m\u001b[1mSetting new control grid for warping field.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "#Dataset = data.CIFAR10\n",
    "Dataset = data.GTSRB\n",
    "#Dataset = data.MNIST\n",
    "\n",
    "model = models.PreActResNet(\n",
    "    block=models.models.PreActBlock,\n",
    "    num_blocks=[2, 2, 2, 2],\n",
    "    num_classes=Dataset.num_classes,\n",
    ")\n",
    "\n",
    "backdoor = data.WanetBackdoor(path=None, p_backdoor=0.1, p_noise=0.2)\n",
    "#backdoor = data.CornerPixelBackdoor(p_backdoor=0.1)\n",
    "#backdoor = data.NoiseBackdoor(p_backdoor=0.1)\n",
    "\n",
    "\n",
    "n_replicas = 1\n",
    "with executor.batch():\n",
    "    for _ in range(n_replicas):\n",
    "        for Dataset in [\n",
    "            data.MNIST,\n",
    "            data.GTSRB,\n",
    "            data.CIFAR10,\n",
    "        ]:\n",
    "            for model in [\n",
    "                models.MLP(\n",
    "                    input_shape=(3, 28, 28) if Dataset == data.MNIST else (3, 32, 32),\n",
    "                    hidden_dims=[128, 128, 128],\n",
    "                    output_dim=Dataset.num_classes,\n",
    "                ),\n",
    "                models.CNN(\n",
    "                    input_shape=(3, 28, 28) if Dataset == data.MNIST else (3, 32, 32),\n",
    "                    channels=[16, 32, 32, 64],\n",
    "                    dense_dims=[128],\n",
    "                    output_dim=Dataset.num_classes,),\n",
    "                models.PreActResNet(\n",
    "                    block=models.models.PreActBlock,\n",
    "                    num_blocks=[2, 2, 2, 2],\n",
    "                    num_classes=Dataset.num_classes,\n",
    "                ),\n",
    "            ]:\n",
    "                for backdoor in [\n",
    "                    data.CornerPixelBackdoor(p_backdoor=0.1),\n",
    "                    data.NoiseBackdoor(p_backdoor=0.1),\n",
    "                    data.WanetBackdoor(path=None, p_backdoor=0.1, p_noise=0.2),\n",
    "                ]:\n",
    "                    job = executor.submit(train_classifier, model=model, Dataset=Dataset, backdoor=backdoor)\n",
    "                    jobs.append(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlurmJob<job_id=2349069_0, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_1, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_2, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_3, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_4, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_5, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_6, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_7, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_8, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_9, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_10, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_11, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_12, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_13, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_14, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_15, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_16, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_17, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_18, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_19, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_20, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_21, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_22, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_23, task_id=0, state=\"COMPLETED\">\n",
      "SlurmJob<job_id=2349069_24, task_id=0, state=\"RUNNING\">\n",
      "SlurmJob<job_id=2349069_25, task_id=0, state=\"RUNNING\">\n",
      "SlurmJob<job_id=2349069_26, task_id=0, state=\"RUNNING\">\n"
     ]
    }
   ],
   "source": [
    "for job in jobs:\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = jobs[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This job can be monitored from: https://scruffy.c3se.chalmers.se/d/alvis-job/alvis-job?var-jobid=2349223&from=1715858707000\n",
      "submitit INFO (2024-05-16 13:25:08,346) - Starting with JobEnvironment(job_id=2349069_23, hostname=alvis3-18, local_rank=0(1), node=0(1), global_rank=0(1))\n",
      "submitit INFO (2024-05-16 13:25:08,347) - Loading pickle: /mimer/NOBACKUP/groups/ml-safety/vikren/mad/cupbearer/runs/2349069_23_submitted.pkl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "submitit INFO (2024-05-16 14:14:26,032) - Job completed successfully\n",
      "submitit INFO (2024-05-16 14:14:26,033) - Exiting after successful completion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(job.stdout())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:25:14.846 | DEBUG    | cupbearer.data.backdoors:control_grid:140 - Generating new control grid for warping field.\n",
      "2024-05-16 13:25:14.859 | DEBUG    | cupbearer.data.backdoors:control_grid:163 - Setting new control grid for warping field.\n",
      "2024-05-16 13:25:14.859 | DEBUG    | cupbearer.data.backdoors:clone:197 - Setting control grid of clone from instance.\n",
      "2024-05-16 13:25:14.859 | DEBUG    | cupbearer.data.backdoors:control_grid:163 - Setting new control grid for warping field.\n",
      "2024-05-16 13:25:15.537 | DEBUG    | cupbearer.data.backdoors:control_grid:140 - Generating new control grid for warping field.\n",
      "2024-05-16 13:25:15.538 | DEBUG    | cupbearer.data.backdoors:control_grid:163 - Setting new control grid for warping field.\n",
      "2024-05-16 13:25:15.538 | DEBUG    | cupbearer.data.backdoors:clone:197 - Setting control grid of clone from instance.\n",
      "2024-05-16 13:25:15.538 | DEBUG    | cupbearer.data.backdoors:control_grid:163 - Setting new control grid for warping field.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params\n",
      "------------------------------------------------------\n",
      "0 | model          | CNN                | 42.4 K\n",
      "1 | train_accuracy | MulticlassAccuracy | 0     \n",
      "2 | val_accuracy   | ModuleList         | 0     \n",
      "3 | test_accuracy  | ModuleList         | 0     \n",
      "------------------------------------------------------\n",
      "42.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "42.4 K    Total params\n",
      "0.170     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/mimer/NOBACKUP/groups/ml-safety/vikren/mad/cupbearer-env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n",
      "2024-05-16 14:14:26.031 | DEBUG    | cupbearer.data.backdoors:store:236 - Storing control grid to logs/CNN/CIFAR10/WanetBackdoor/2349223/wanet_backdoor.pt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(job.stderr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
